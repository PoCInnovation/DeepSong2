{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import librosa\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datas\n",
    "\n",
    "TO DO: Find the data, this could be midi songs from [Lakh MIDI Dataset](https://colinraffel.com/projects/lmd/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datas with librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DataFrame' has no attribute 'recording_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sample_num\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m \u001b[39m#pick a file to display\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#get the filename \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m filename\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39;49mrecording_id[sample_num]\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m.flac\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#define the beginning time of the signal\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tstart \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mt_min[sample_num] \n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DataFrame' has no attribute 'recording_id'"
     ]
    }
   ],
   "source": [
    "y,sr=librosa.load('train/'+str(filename)) #load the file\n",
    "librosa.display.waveplot(y,sr=sr, x_axis='time', color='cyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate feature to feed vq-vae (batch_size, heigh, width, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(y_cut):\n",
    "    max_size=1000 #my max audio file feature width\n",
    "    height_size = 128\n",
    "    stft = utils.padding(np.abs(librosa.stft(y_cut, n_fft=255, hop_length=512)), height_size, max_size) # adjust n_fft to physical duration\n",
    "    MFCCs = utils.padding(librosa.feature.mfcc(y_cut, n_fft=librosa.n_fft, hop_length=librosa.hop_length, n_mfcc=128),\n",
    "                          height_size, max_size)\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=y_cut, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y_cut, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y_cut, sr=sr)    #Now the padding part\n",
    "    image = np.array([utils.padding(normalize(spec_bw),1, max_size)]).reshape(1,max_size)\n",
    "    image = np.append(image, utils.padding(normalize(spec_centroid), 1, max_size), axis=0) #repeat the padded spec_bw,spec_centroid and chroma stft until they are stft and MFCC-sized\n",
    "    for i in range(0,9):\n",
    "        image = np.append(image, utils.padding(normalize(spec_bw), 1, max_size), axis=0)\n",
    "        image = np.append(image, utils.padding(normalize(spec_centroid), 1, max_size), axis=0)\n",
    "        image = np.append(image, utils.padding(normalize(chroma_stft), 12, max_size), axis=0)\n",
    "    image=np.dstack((image,np.abs(stft)))\n",
    "    image=np.dstack((image, MFCCs))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantizer Layer\n",
    "\n",
    "Les embeddings (vecteurs ou tensor) vont être quantifier (placés) dans le codebook (dictionnaire) pour créer un espace latenet de vecteurs discrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # flatern input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "\n",
    "        # calculate euclidian distances (sum(A^2) + sum(B^2) - 2 * dot(A, B))\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "\n",
    "        # Find the closest codebook vectors and their indices\n",
    "        indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "        # Look up the corresponding embeddings from the codebook\n",
    "        quantized = self._embedding(indices).view_as(inputs)\n",
    "\n",
    "        # Loss (compute mse from quantized vectors to input vectors and from input vectors to quantized vec)\n",
    "        # detach is used to prevent gradients from flowing into the embedding vectors\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "\n",
    "        # Used to keep relation between quantized and input vectors\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(indices.float(), dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "TO DO: Find a good way to do a bottleneck on input data. A solution could be to copy the [ss-vq-vae encoder](https://arxiv.org/pdf/2102.05749.pdf) (2x conv[4, 2], conv[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1024, 1024, 4, 2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(1024, 1024, 4, 2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(1024, 1024, 1, 1),\n",
    "       )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDecoder\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/degodenne/Bureau/Poc/DeepSong2/vq_vae.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39msuper\u001b[39m(Encoder, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1024, 1024, 1, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.GRU(1024, 1024),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(1024, 1024, 4, 2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(1024, 1024, 1, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.GRU(1024, 1024),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(1024, 1024, 4, 2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(1025, 1025, 1, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.GRU(1025, 1025),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, \n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self._encoder = Encoder()\n",
    "#       self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, \n",
    "#                                     out_channels=embedding_dim,\n",
    "#                                     kernel_size=1, \n",
    "#                                     stride=1)\n",
    "        self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
